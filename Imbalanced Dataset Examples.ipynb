{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ff1b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db9a8f",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "1) relying on \"predict\" to predict: instead, use the decision function and tune a decision threshold\n",
    "- experiment: a variety of dataset imbalance levels.  compare:\n",
    "    - raw data, use \"predict\"\n",
    "    - balanced data, use \"predict\"\n",
    "    - raw data - decision function and tune threshold\n",
    "\n",
    "\n",
    "2) sticking with algorithm defaults\n",
    "- experiment: use very imbalanced data, plot log loss as a function of number iterations\n",
    "    - raw data - default base score\n",
    "    - raw data - pos frac base score\n",
    "    \n",
    "    \n",
    "3) resampling or reweighting to control output class balance\n",
    "- highly imbalanced dataset - no positives coming out at first\n",
    "    - raw data - no positives\n",
    "    - raw data again - just use decision function + threshold\n",
    "    \n",
    "   \n",
    "4) tuning and evaluating on resampled datasets: DONE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fa752",
   "metadata": {},
   "source": [
    "# Imbalanced Dataset Experiments\n",
    "\n",
    "This is a quick and simple demonstration of the type of error which can result from improper use of resampling to balance a dataset before training a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d0338",
   "metadata": {},
   "source": [
    "We use a simple toy dataset using scikit-learn's \"make_classification\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32676a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.524584</td>\n",
       "      <td>0.412494</td>\n",
       "      <td>-1.802713</td>\n",
       "      <td>-0.075889</td>\n",
       "      <td>0.744249</td>\n",
       "      <td>-0.555693</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>-2.902355</td>\n",
       "      <td>-1.032001</td>\n",
       "      <td>0.620666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.223714</td>\n",
       "      <td>0.353656</td>\n",
       "      <td>0.433975</td>\n",
       "      <td>-1.692863</td>\n",
       "      <td>1.798698</td>\n",
       "      <td>-0.493160</td>\n",
       "      <td>0.735316</td>\n",
       "      <td>-2.147366</td>\n",
       "      <td>0.918143</td>\n",
       "      <td>-0.925999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.375770</td>\n",
       "      <td>0.091193</td>\n",
       "      <td>-1.311360</td>\n",
       "      <td>-0.922734</td>\n",
       "      <td>-1.721759</td>\n",
       "      <td>2.272621</td>\n",
       "      <td>0.543799</td>\n",
       "      <td>3.121716</td>\n",
       "      <td>-1.228138</td>\n",
       "      <td>-1.326703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.040756</td>\n",
       "      <td>0.818655</td>\n",
       "      <td>1.231301</td>\n",
       "      <td>1.584360</td>\n",
       "      <td>-2.103652</td>\n",
       "      <td>0.939735</td>\n",
       "      <td>-0.770357</td>\n",
       "      <td>1.072464</td>\n",
       "      <td>-0.521160</td>\n",
       "      <td>1.035603</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.716362</td>\n",
       "      <td>0.052499</td>\n",
       "      <td>-0.947138</td>\n",
       "      <td>1.301603</td>\n",
       "      <td>-0.710176</td>\n",
       "      <td>-0.206302</td>\n",
       "      <td>0.100330</td>\n",
       "      <td>-0.332693</td>\n",
       "      <td>-3.112213</td>\n",
       "      <td>-0.955480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  2.524584  0.412494 -1.802713 -0.075889  0.744249 -0.555693  0.179100   \n",
       "1  1.223714  0.353656  0.433975 -1.692863  1.798698 -0.493160  0.735316   \n",
       "2 -2.375770  0.091193 -1.311360 -0.922734 -1.721759  2.272621  0.543799   \n",
       "3 -0.040756  0.818655  1.231301  1.584360 -2.103652  0.939735 -0.770357   \n",
       "4  0.716362  0.052499 -0.947138  1.301603 -0.710176 -0.206302  0.100330   \n",
       "\n",
       "          7         8         9  target  \n",
       "0 -2.902355 -1.032001  0.620666       0  \n",
       "1 -2.147366  0.918143 -0.925999       0  \n",
       "2  3.121716 -1.228138 -1.326703       0  \n",
       "3  1.072464 -0.521160  1.035603       1  \n",
       "4 -0.332693 -3.112213 -0.955480       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_toy_dataset(**kwargs):\n",
    "    ''' the characteristics of the dataset affect the degree to which we see this issue manifest\n",
    "        easily separable datasets with confidently correct models may not have the same issue as \n",
    "        noisier datasets with less confident models\n",
    "    '''\n",
    "    args = dict(\n",
    "        weights=[0.9,0.1], # Setting weights so classes are unbalanced\n",
    "        n_samples=100000,\n",
    "        n_features=10,\n",
    "        n_informative=3,\n",
    "        n_redundant=2,\n",
    "        n_classes=2,\n",
    "        class_sep=0.5,\n",
    "        random_state=42,\n",
    "    )\n",
    "    args.update(**kwargs)\n",
    "    \n",
    "    # Generating an unbalanced dataset\n",
    "    X, y = make_classification(**args)\n",
    "    df = pd.DataFrame(X)\n",
    "    df.columns = df.columns.astype(str)\n",
    "    df['target'] = y\n",
    "    return df\n",
    "\n",
    "generate_toy_dataset().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be68fe",
   "metadata": {},
   "source": [
    "# Experiment: Evaluating on resampled datasets\n",
    "\n",
    "This experiment trains and applies a random forest model both with and without resampling the training set.\n",
    "\n",
    "In each of the 2 experiments, a test set is first broken off and set aside to simulate \"production data\" that the model will be applied to in the future.\n",
    "\n",
    "For the \"resampling\" experiment, we use a downsampler.  If you like, you can try this with an upsampler as well.  This will take longer to run and tends to make the results slightly better and less variable but should not change the overall conclusion. \n",
    "\n",
    "For the \"non resampled\" experiment, we just feed the data in as-is without and modification.\n",
    "\n",
    "The experiment:\n",
    "    - resample the data (or not)\n",
    "    - break the data into train and validation sets\n",
    "    - train a model on the training set\n",
    "    - optimize a decision threshold using the validation set\n",
    "    - apply the model with this threshold to the \"production\" dataset\n",
    "    - evaluate the f1 statistic on the \"production\" dataset and compare to the results on the validation set\n",
    "    \n",
    "Do this twice, and compare the results (resampling vs not resampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d360a994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_threshold(probas, target):\n",
    "    ''' iterates over thresholds to choose the one with the highest f1 score '''\n",
    "    results = dict()\n",
    "    precision, recall, thresholds = precision_recall_curve(target, probas)\n",
    "    f1_scores = 2 * recall * precision / (recall + precision)\n",
    "    best_f1_ind = np.argmax(f1_scores)\n",
    "    results['best_threshold'] = thresholds[best_f1_ind]\n",
    "    results['validation_precision']      = precision[best_f1_ind]\n",
    "    results['validation_recall']         = recall[best_f1_ind]\n",
    "    results['validation_f1_score']  = np.max(f1_scores)\n",
    "    return results \n",
    "    \n",
    "def train_model_and_do_validation_predict(df_train, df_valid, target):\n",
    "    ''' trains the model and predicts on validation set\n",
    "        returns model and validation set predictions'''\n",
    "    X, y = df_train.drop(columns=target), df_train[target]\n",
    "    X_valid = df_valid.drop(columns=target)\n",
    "    model = RandomForestClassifier(n_jobs=-1, n_estimators=256, min_samples_leaf=20, random_state=42).fit(X, y)\n",
    "    return model, model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "def score_production_predictions(df_prod, target, model, threshold):\n",
    "    ''' computes the f1 score on the \"production\" dataset with the provided threshold '''\n",
    "    X_prod = df_prod.drop(columns=target)\n",
    "    P_prod = model.predict_proba(X_prod)[:, 1] > threshold\n",
    "    return f1_score(df_prod[target], P_prod)\n",
    "\n",
    "\n",
    "def train_optimize_and_predict_on_prod(train_val, df_prod, target):\n",
    "    ''' the full test, validate, tune threhsold, and predict on \"production data\" sequence '''\n",
    "    train, val = train_test_split(train_val, test_size=0.2, random_state=42)\n",
    "    model, P_val = train_model_and_do_validation_predict(train, val, target)\n",
    "    results = choose_best_threshold(P_val, val[target])\n",
    "    prod_f1_score = score_production_predictions(df_prod, target, model, results['best_threshold'])\n",
    "    results['prod_f1_score'] = prod_f1_score\n",
    "    return results\n",
    "\n",
    "def run_experiment(df, target):\n",
    "    # set aside test set for final evaluation\n",
    "    #     held out \"test set\" is called \"df_prod\" to denote that it's\n",
    "    #     our best representation of what the model will be encountering \"in prod\"\n",
    "    #     having the \"untouched\" class balance \n",
    "    train_val, df_prod = train_test_split(df, test_size=0.2, random_state=42) \n",
    "\n",
    "    # resampler - try different resamplers if you like\n",
    "    resampler = RandomUnderSampler(sampling_strategy=1.0, random_state=42) # 50/50 balanced undersampling\n",
    "    train_val_resampled, _ = resampler.fit_sample(train_val, train_val[target])\n",
    "        \n",
    "    # experiment 1: \n",
    "    #  - train model on resampled data\n",
    "    #  - pick optimal threshold based on validation set f1 score\n",
    "    #  - predict on \"production_data\" and compare to our expectations\n",
    "    #    as defined by validation set results\n",
    "    resampled_results = train_optimize_and_predict_on_prod(train_val_resampled, df_prod, target)\n",
    "    \n",
    "    # experiment 2: \n",
    "    #   - same as experiment 1 but do not resample the data first\n",
    "    raw_results = train_optimize_and_predict_on_prod(train_val, df_prod, target)\n",
    "    \n",
    "    results = pd.DataFrame([resampled_results, raw_results], index=['resampled', 'non-resampled']).T.round(2)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651e271f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resampled</th>\n",
       "      <th>non-resampled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>best_threshold</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation_precision</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation_recall</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation_f1_score</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prod_f1_score</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      resampled  non-resampled\n",
       "best_threshold             0.43           0.21\n",
       "validation_precision       0.74           0.42\n",
       "validation_recall          0.84           0.60\n",
       "validation_f1_score        0.79           0.50\n",
       "prod_f1_score              0.38           0.49"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(df=generate_toy_dataset(), target='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4865f01",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Resampling:\n",
    "- estimated f1 (based on validation set) = 79%\n",
    "- actual f1 (based on \"production data\") = 38%\n",
    "\n",
    "No Resampling:\n",
    "- estimated f1 (based on validation set) = 50%\n",
    "- actual f1 (based on \"production data\") = 49%\n",
    "\n",
    "Using a resampled validation set  inflated the validation performance statistics of the model (79% f1) while delivering a much lower performance on the actual dataset (38% f1).  This is one of the biggest risks in making this error - that the model will be deployed under the expectation of substantially greater performance than is actually attained on production data post-deployment.\n",
    "\n",
    "Compare this to the non-resampled dataset.  The actual performance on \"production data\" is much better (49% compared to 38%) and the estimated f1 based on the dataset is much closer to the production performance (50% on the validation set vs 49% on the production set).\n",
    "\n",
    "This is a common phenomenon when care isn't taken while using resampling.  This is only one of several related types of errors resampling can cause you to make if it is not implemented carefully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6173f28e",
   "metadata": {},
   "source": [
    "# Experiment: Relying on \"predict\" to predict\n",
    "\n",
    "1) relying on \"predict\" to predict: instead, use the decision function and tune a decision threshold\n",
    "- experiment: a variety of dataset imbalance levels.  compare:\n",
    "    - raw data, use \"predict\"\n",
    "    - balanced data, use \"predict\"\n",
    "    - raw data - decision function and tune threshold\n",
    "    \n",
    "    \n",
    "    \n",
    "Often, DS's may make use of the default \"predict\" function to extract predictions from their models.  This isn't always bad, but does often lead to substantially sub-optimal performance when something better may have been within easy reach.\n",
    "\n",
    "When you use \"predict\", you get whatever the model hands you.  \n",
    "\n",
    "If you use your own decision threshold, you can choose to optimize the f1 score, the precision at a recall threshold, the recall at a precision threshold, hit specific recall or precision targets or whatever you like. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79367cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     17918\n",
      "           1       0.82      0.09      0.16      2082\n",
      "\n",
      "    accuracy                           0.90     20000\n",
      "   macro avg       0.86      0.54      0.55     20000\n",
      "weighted avg       0.90      0.90      0.87     20000\n",
      "\n",
      "optimize f1 score\n",
      "threshold: 0.1691846899752046\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.85      0.90     17918\n",
      "           1       0.33      0.62      0.43      2082\n",
      "\n",
      "    accuracy                           0.83     20000\n",
      "   macro avg       0.64      0.74      0.66     20000\n",
      "weighted avg       0.89      0.83      0.85     20000\n",
      "\n",
      "optimize precision at recall=0.5\n",
      "threshold: 0.21425225103961543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92     17918\n",
      "           1       0.37      0.51      0.43      2082\n",
      "\n",
      "    accuracy                           0.86     20000\n",
      "   macro avg       0.66      0.71      0.67     20000\n",
      "weighted avg       0.88      0.86      0.87     20000\n",
      "\n",
      "optimize recall at precision=0.7\n",
      "threshold: 0.4258855798962829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95     17918\n",
      "           1       0.69      0.11      0.19      2082\n",
      "\n",
      "    accuracy                           0.90     20000\n",
      "   macro avg       0.80      0.55      0.57     20000\n",
      "weighted avg       0.88      0.90      0.87     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def relying_on_predict_to_predict():\n",
    "    \n",
    "    def threshold_which_optimizes_recall_at_precision(precision_min=0.7):\n",
    "        p_inds = precision > precision_min\n",
    "        t = thresholds[p_inds[:-1]][0]\n",
    "        print('threshold:', t)\n",
    "        return t \n",
    "        \n",
    "    def threshold_which_optimizes_precision_at_recall(recall_min=0.5):\n",
    "        r_inds = recall > recall_min\n",
    "        t = thresholds[r_inds[:-1]][-1]\n",
    "        print('threshold:', t)\n",
    "        return t\n",
    "        \n",
    "    def threshold_which_optimizes_f1_score():\n",
    "        f1_scores = 2 * recall * precision / (recall + precision)\n",
    "        best_f1_ind = np.argmax(f1_scores)\n",
    "        t = thresholds[best_f1_ind]\n",
    "        print('threshold:', t)\n",
    "        return t\n",
    "\n",
    "    df = generate_toy_dataset(class_sep=0.3, n_samples=100000)\n",
    "    target = 'target'\n",
    "    \n",
    "    train_val, df_prod = train_test_split(df, test_size=0.2, random_state=42) \n",
    "    df_train, df_valid         = train_test_split(train_val, test_size=0.2, random_state=42) \n",
    "\n",
    "    X, y = df_train.drop(columns=target), df_train[target]\n",
    "    X_valid, y_valid = df_valid.drop(columns=target), df_valid[target]\n",
    "    X_prod,  y_prod  = df_prod.drop(columns=target),  df_prod[target]\n",
    "    model = RandomForestClassifier(n_jobs=-1, n_estimators=256, min_samples_leaf=20, random_state=42).fit(X, y)\n",
    "    \n",
    "    proba_valid = model.predict_proba(X_valid)[:, 1]\n",
    "    P_valid     = model.predict(X_valid)\n",
    "    proba_prod  = model.predict_proba(X_prod)[:, 1]\n",
    "    P_prod      = model.predict(X_prod)\n",
    "\n",
    "    # method 1: use \"predict\" function:\n",
    "    print(classification_report(y_prod, P_prod))\n",
    "    \n",
    "    \n",
    "    # method 2: use decision function (predict_proba) and optimize\n",
    "    #           threshold on validation set; use threshold on test set\n",
    "    #           to check quality\n",
    "    precision, recall, thresholds = precision_recall_curve(y_valid, proba_valid)\n",
    "    print('optimize f1 score')\n",
    "    print(classification_report(y_prod, proba_prod>threshold_which_optimizes_f1_score()))\n",
    "    \n",
    "    print('optimize precision at recall=0.5')\n",
    "    print(classification_report(y_prod, proba_prod>threshold_which_optimizes_precision_at_recall()))\n",
    "    \n",
    "    print('optimize recall at precision=0.7')\n",
    "    print(classification_report(y_prod, proba_prod>threshold_which_optimizes_recall_at_precision()))\n",
    "\n",
    "    \n",
    "relying_on_predict_to_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ecdcc0",
   "metadata": {},
   "source": [
    "The default \"predict\" function sticks us with a relatively high precision but a pretty low recall and a pretty low f1-score of 0.16.\n",
    "\n",
    "If we use predict_proba with a decision threshold of ~0.17 (vs the original default of 0.5)- we can achieve an f1 of 0.43 compared to the original 0.16.  That's a substantial difference.\n",
    "\n",
    "Similarly - if we were concerned with catching a larger fraction of these positives and decided that 50% was good enough, we can achieve that while hitting a precision of 0.37.\n",
    "\n",
    "Likewise, we could hit a target precision of 70% (if, say, we didn't want to waste time on too many false positives) and achieve an 11% recall.  \n",
    "\n",
    "A whole menu of options for sculpting model behavior opens up here once we move into the realm of decision functions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d092b105",
   "metadata": {},
   "source": [
    "# Experiment: Sticking with algorithm defaults\n",
    "\n",
    "Let's explore my favorite example of algorithm defaults which result in worse performance on imbalanced datasets: XGBoost's default \"0th\" constant estimator. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
